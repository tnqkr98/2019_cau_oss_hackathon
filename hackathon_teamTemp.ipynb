{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackathon_teamTemp",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnqkr98/2019_cau_oss_hackathon/blob/master/hackathon_teamTemp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc",
        "colab_type": "text"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        " *  단, 사용할 데이터셋에 따라 is_mnist만 수정\n",
        "*   모든 구현은 [2. 데이터 전처리]와 [3. 모델 생성]에서만 진행\n",
        " *  데이터 전처리 후 트레이닝, 데이터 셋은 x_train_after, x_test_after 변수명을 유지해주세요.\n",
        " *  데이터셋이 달라져도 같은 모델 구조를 유지하여야함.\n",
        "*   [4. 모델 저장]과 [5. 모델 로드 및 평가]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *  트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        " *  team_name을 제외한 다른 부분은 수정하지 말 것\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  모델 구조 파일 (model_structure_teamXX.json)\n",
        " *  모델 weight 파일 (model_weight_teamXX.h5)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        "* 제출 기한: **오후 6시**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2019_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  두 개의 데이터셋에 대해 다른 모델 구조를 사용한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1",
        "colab_type": "text"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "f95c2e06-925e-40a5-cb97-c38f40825694"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from keras import regularizers\n",
        "\n",
        "is_mnist = True;\n",
        "\n",
        "# 데이터셋 로드\n",
        "# x_train, y_train: 트레이닝 데이터 및 레이블\n",
        "# x_test, y_test: 테스트 데이터 및 레이블\n",
        "if is_mnist:\n",
        "  data_type = 'mnist'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # fashion MNIST 데이터셋인 경우,\n",
        "else:\n",
        "  data_type = 'cifar10'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # cifar10 데이터셋인 경우,\n",
        "\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test.shape[1:]\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9c2KLDBIhNQ",
        "colab_type": "text"
      },
      "source": [
        "# **2. 데이터 전처리**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJNgjaHvIhSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "92f1a968-6f76-479b-c214-c473358ab244"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "\n",
        "# Mnist 데이터 증폭.\n",
        "\n",
        "\n",
        "x_train3 = []  \n",
        "y_train3 = []  \n",
        "\n",
        "\n",
        "for i, x in enumerate(x_train):   # 학습데이터 늘리기2 . MedianBlur\n",
        "    # print(x)\n",
        "    x_train3.append(x)\n",
        "    y_train3.append(y_train[i])\n",
        "\n",
        "\n",
        "    blur = cv2.medianBlur(x, 3)\n",
        "    x_train3.append(blur)\n",
        "    y_train3.append(y_train[i])\n",
        "\n",
        "    blur2 = cv2.GaussianBlur(x, (3,3),0)\n",
        "    x_train3.append(blur2)\n",
        "    y_train3.append(y_train[i])\n",
        "\n",
        "\n",
        "    fm = cv2.flip(x,1)\n",
        "    x_train3.append(fm)\n",
        "    y_train3.append(y_train[i])\n",
        "\n",
        "    im = Image.fromarray(np.uint8(x))\n",
        "    for ang in range(-6,6,3):\n",
        "        tm = im.rotate(ang)\n",
        "        #print(type(tm))\n",
        "        #cv2.imwrite(\"rot1.png\",np.asarray(tm))\n",
        "        #break\n",
        "        x_train3.append(np.asarray(tm))\n",
        "        y_train3.append(y_train[i])\n",
        "\n",
        "\n",
        "\n",
        "x_train3 = np.asarray(x_train3)\n",
        "y_train3 = np.asarray(y_train3)\n",
        "\n",
        "\n",
        "print(x_train3.shape)\n",
        "print(y_train3.shape)\n",
        "\n",
        "\n",
        "##x_train_after = x_train3 \n",
        "##x_test_after = x_test \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(480000, 28, 28)\n",
            "(480000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufq1mI0PmO7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "x_train_after = x_train3 \n",
        "x_test_after = x_test \n",
        "\n",
        "if is_mnist:\n",
        "  x_train_after = np.reshape(x_train_after,(-1,28,28,1))\n",
        "  x_test_after = np.reshape(x_test_after, (-1,28,28,1))\n",
        "\n",
        "# normalize by mean and std\n",
        "mean = np.mean(x_train_after, axis=(0, 1, 2, 3))\n",
        "std = np.std(x_train_after, axis=(0, 1, 2, 3))\n",
        "x_train_after = (x_train_after - mean) / (std + 1e-7)\n",
        "x_test_after = (x_test_after - mean) / (std + 1e-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9",
        "colab_type": "text"
      },
      "source": [
        "# **3. 모델 생성**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8coOY-DzUDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "335cf72d-abda-4c72-efe1-c6c10aa0931f"
      },
      "source": [
        "# 순차 모델 생성 (가장 기본구조)\n",
        "model = keras.Sequential()\n",
        "weight_decay = 1e-4\n",
        "# ------\n",
        "# Conv1\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_regularizer=regularizers.l2(weight_decay), input_shape=(np.shape(x_train_after[0]))))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPool2D(2,2))\n",
        "model.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "# ------\n",
        "# Conv2\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPool2D(2,2))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "# ------\n",
        "# Conv3\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=1, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=1, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(keras.layers.MaxPool2D(2,2))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# Output layer: fully-connected layer \n",
        "# (# of inputs = 64, # of outputs = 10, actication fuction = softmax)\n",
        "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "optm = keras.optimizers.Adam(1e-3, decay=1e-8)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optm, metrics=['accuracy'])\n",
        "\n",
        "# #augmentation\n",
        "# #data augmentation\n",
        "# save_path = '/content/'\n",
        "# team_name = 'teamXX'\n",
        "# checkpoint = ModelCheckpoint(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5', monitor=\"val_acc\", save_best_only=True, mode=\"max\", period=1)\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-4)\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=10,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     horizontal_flip=True,\n",
        "#     )\n",
        "# datagen.fit(x_train_after)\n",
        "# model.fit_generator(datagen.flow(x_train_after, y_train, batch_size=256),epochs=100,verbose=1,validation_data=(x_test_after,y_test), shuffle=True,callbacks=[reduce_lr,checkpoint])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "save_path = '/content/'\n",
        "team_name = 'teamTemp'\n",
        "checkpoint = ModelCheckpoint(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5', monitor=\"val_acc\", save_best_only=True, mode=\"max\", period=1,verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-4)\n",
        "model.summary()\n",
        "model.fit(x_train_after, y_train3, batch_size = 1024, verbose=1, epochs = 100, shuffle=True, validation_data=[x_test_after, y_test],callbacks=[reduce_lr,checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 08:49:41.470127 139848474634112 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                11530     \n",
            "=================================================================\n",
            "Total params: 299,498\n",
            "Trainable params: 298,730\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Train on 480000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "479232/480000 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.8217\n",
            "Epoch 00001: val_acc improved from -inf to 0.80930, saving model to /content/model_weight_mnist_teamTemp.h5\n",
            "480000/480000 [==============================] - 57s 118us/sample - loss: 0.5414 - acc: 0.8217 - val_loss: 0.5476 - val_acc: 0.8093\n",
            "Epoch 2/100\n",
            "479232/480000 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8939\n",
            "Epoch 00002: val_acc improved from 0.80930 to 0.91360, saving model to /content/model_weight_mnist_teamTemp.h5\n",
            "480000/480000 [==============================] - 51s 106us/sample - loss: 0.3246 - acc: 0.8939 - val_loss: 0.2730 - val_acc: 0.9136\n",
            "Epoch 3/100\n",
            "479232/480000 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.9082\n",
            "Epoch 00003: val_acc improved from 0.91360 to 0.91870, saving model to /content/model_weight_mnist_teamTemp.h5\n",
            "480000/480000 [==============================] - 51s 107us/sample - loss: 0.2854 - acc: 0.9082 - val_loss: 0.2659 - val_acc: 0.9187\n",
            "Epoch 4/100\n",
            "479232/480000 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9170\n",
            "Epoch 00004: val_acc improved from 0.91870 to 0.92820, saving model to /content/model_weight_mnist_teamTemp.h5\n",
            "480000/480000 [==============================] - 51s 107us/sample - loss: 0.2627 - acc: 0.9169 - val_loss: 0.2455 - val_acc: 0.9282\n",
            "Epoch 5/100\n",
            "479232/480000 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9219\n",
            "Epoch 00005: val_acc improved from 0.92820 to 0.93470, saving model to /content/model_weight_mnist_teamTemp.h5\n",
            "480000/480000 [==============================] - 51s 106us/sample - loss: 0.2497 - acc: 0.9219 - val_loss: 0.2286 - val_acc: 0.9347\n",
            "Epoch 6/100\n",
            "384000/480000 [=======================>......] - ETA: 10s - loss: 0.2421 - acc: 0.9246"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR",
        "colab_type": "text"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'teamTemp'\n",
        "\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2BoRDZ7cFl",
        "colab_type": "text"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDBwxVUx7knQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "61633a29-7e67-4d31-e2ad-29b2b3938589"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'teamTemp'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0824 08:48:43.314858 139848474634112 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0824 08:48:43.316372 139848474634112 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0824 08:48:43.338045 139848474634112 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 148us/sample - loss: 0.4355 - acc: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.43549083728790283, 0.8818]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSgqEQ13mgG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}